{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f7143b-c7e8-40c4-a112-9931888c44f1",
   "metadata": {},
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178f6d70-28d5-4794-ae47-8adac62dc51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577883d4-969f-4e2c-9b2a-3cab226903b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ae993-dab1-4f7b-a855-34f634f10421",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d72a09-6eac-436d-aff1-886988ec7924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/johny02/projects/local-rag-chat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8514a843-ad7c-4d11-825c-0517f9212d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/johny02/projects/local-rag-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johny02/anaconda3/envs/local_rag_chat/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /home/johny02/projects/local-rag-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382c4f85-ba57-40c9-9a56-f1a65a48027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p 'data/paul_graham/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65fd60e9-8b57-4ef7-ac14-4c7af79123e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-16 15:06:42--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-12-16 15:06:43 (1.64 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072fb1d-46f1-4308-bea4-dc7a08bbca74",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7c4cf782-8173-408b-bf5a-e025f7d294d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"./local_rag_chat/eval/data/paul_graham\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dabfd8f-58d6-4589-95bc-cce998f857e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 15:21:27,647 [INFO] app_logger: Model llama3.2 does not exist. Pulling model...\n",
      "2024-12-18 15:21:30,014 [INFO] app_logger: Loaded Ollama model: llama3.2\n"
     ]
    }
   ],
   "source": [
    "from local_rag_chat.core.llms.ollama import OllamaModel\n",
    "llm = OllamaModel(model='llama3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad10bb3-0a03-47e6-85f4-a2bc7933a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DatasetGenerator.from_documents(documents, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891063a-e562-48c1-ae88-1c7ac7f7bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ad5cc5e-5224-4e71-ae92-65bc6c0c31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('local_rag_chat/eval/data/paul_graham/questions.txt', 'w') as f:\n",
    "    for question in eval_questions:\n",
    "        f.write(f'{question.strip()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a4bf81-7ae4-44b6-8650-5d657a77df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('local_rag_chat/eval/data/paul_graham/questions.txt', 'r') as f:\n",
    "    questions = f.readlines()\n",
    "    eval_questions = [line.rstrip() for line in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97091db6-c8c6-456c-86b4-b671e9139ec6",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cec64-25b6-4c39-9ff4-a5900b304cf6",
   "metadata": {},
   "source": [
    "**Hit Rate:**\n",
    "\n",
    "Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n",
    "\n",
    "**Mean Reciprocal Rank (MRR):**\n",
    "\n",
    "For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\n",
    "\n",
    "Let's check these metrics to check the performance of out retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c3512-2a72-44e1-802e-47d99a574bf1",
   "metadata": {},
   "source": [
    "## Base Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6e3cb1b-4e5b-4cc2-b105-b40d7d2d96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3fa46d08-887d-489f-b6d8-ac1f3f38c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "830132e5-da62-4956-82d3-c6b5513518c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d258b740-d491-425f-a1c0-8efd9dbe8e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "73c0f7b8-1086-4df4-995a-4a0946dc5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "482b5486-f258-42bb-9476-78ea6bf65f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.50it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(\"What was the primary reason you stopped using your time, when writing short stories and programming, after the introduction of microcomputers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d9e9d194-2abf-4b69-9bb0-41c807857fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 3458ff03-e8b2-4275-beff-eda0af5da61c<br>**Similarity:** 0.7235259160039743<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 742915ba-3ca0-4606-a3d4-9473b4c4ae15<br>**Similarity:** 0.7182219860425836<br>**Text:** I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n",
       "\n",
       "With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 85995b93-ee69-4f4f-8aec-2eca7d17fe64<br>**Similarity:** 0.7055373393296052<br>**Text:** I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\n",
       "\n",
       "Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n",
       "\n",
       "Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledg...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 8ea39fa0-52db-40b1-89b2-34796b6eac28<br>**Similarity:** 0.6816770476743171<br>**Text:** What were the two main things the author worked on before college?\n",
       "How did the author describe their early attempts at writing short stories?\n",
       "What type of computer did the author first work on for programming?\n",
       "What language did the author use for programming on the IBM 1401?\n",
       "What was the author's experience with programming on the 1401?\n",
       "What type of computer did the author eventually get for themselves?\n",
       "What was the author's initial plan for college?\n",
       "What made the author change their mind about studying philosophy?\n",
       "What sparked the author's interest in AI?\n",
       "What did the author realize about AI during their first year of grad school?\n",
       "What were the two art schools that the author applied to?\n",
       "How did the author end up at RISD?\n",
       "What was the purpose of the foundation classes at RISD?\n",
       "How did the author manage to pass the entrance exam for the Accademia di Belli Arti?\n",
       "What was the arrangement between the students and faculty at the Accademia?\n",
       "What was the author's experience painting still...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 81d23718-4cc0-4a2e-8076-e333b7145318<br>**Similarity:** 0.6689068279918875<br>**Text:** Now lots of startups get their initial set of customers almost entirely from among their batchmates.\n",
       "\n",
       "I had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited about it, it started to take up a lot more than a third of my attention. But for the first few years I was still able to work on other things.\n",
       "\n",
       "In the summer of 2006, Robert and I started working on a new version of Arc. This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing but startups. Plus it wasn't startup founders we wanted to reach. It was future startup founders. So I changed the name to Hacker News and the topic to whatever engaged one's intellectual curiosity.\n",
       "\n",
       "HN was no doubt good for YC, but it was ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ee6bdaa6-2e2a-40fd-b698-a75ddf220a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/65 [00:00<?, ?it/s]2024-12-17 17:54:17,910 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  2%|▋                                           | 1/65 [00:29<31:09, 29.22s/it]2024-12-17 17:54:45,351 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  3%|█▎                                          | 2/65 [00:56<29:34, 28.17s/it]2024-12-17 17:55:08,749 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  5%|██                                          | 3/65 [01:20<26:51, 25.99s/it]2024-12-17 17:55:30,901 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  6%|██▋                                         | 4/65 [01:42<24:53, 24.48s/it]2024-12-17 17:55:54,870 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  8%|███▍                                        | 5/65 [02:06<24:17, 24.29s/it]2024-12-17 17:56:20,403 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  9%|████                                        | 6/65 [02:31<24:18, 24.71s/it]2024-12-17 17:56:42,732 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 11%|████▋                                       | 7/65 [02:54<23:08, 23.93s/it]2024-12-17 17:57:05,768 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 12%|█████▍                                      | 8/65 [03:17<22:27, 23.65s/it]2024-12-17 17:57:28,824 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 14%|██████                                      | 9/65 [03:40<21:53, 23.46s/it]2024-12-17 17:57:52,263 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 15%|██████▌                                    | 10/65 [04:03<21:30, 23.46s/it]2024-12-17 17:58:15,906 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 17%|███████▎                                   | 11/65 [04:27<21:09, 23.51s/it]2024-12-17 17:58:37,647 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 18%|███████▉                                   | 12/65 [04:48<20:17, 22.97s/it]2024-12-17 17:58:56,820 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 20%|████████▌                                  | 13/65 [05:08<18:54, 21.82s/it]2024-12-17 17:59:21,010 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 22%|█████████▎                                 | 14/65 [05:32<19:09, 22.54s/it]2024-12-17 17:59:43,458 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 23%|█████████▉                                 | 15/65 [05:54<18:45, 22.51s/it]2024-12-17 18:00:06,554 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 25%|██████████▌                                | 16/65 [06:17<18:31, 22.69s/it]2024-12-17 18:00:31,612 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 26%|███████████▏                               | 17/65 [06:42<18:43, 23.40s/it]2024-12-17 18:00:52,152 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 28%|███████████▉                               | 18/65 [07:03<17:39, 22.54s/it]2024-12-17 18:01:14,597 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 29%|████████████▌                              | 19/65 [07:25<17:15, 22.51s/it]2024-12-17 18:01:39,655 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 31%|█████████████▏                             | 20/65 [07:50<17:27, 23.28s/it]2024-12-17 18:02:10,198 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 32%|█████████████▉                             | 21/65 [08:21<18:40, 25.46s/it]2024-12-17 18:02:42,193 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 34%|██████████████▌                            | 22/65 [08:53<19:39, 27.42s/it]2024-12-17 18:03:08,442 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 35%|███████████████▏                           | 23/65 [09:19<18:56, 27.07s/it]2024-12-17 18:03:36,253 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 37%|███████████████▉                           | 24/65 [09:47<18:38, 27.29s/it]2024-12-17 18:03:58,680 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 38%|████████████████▌                          | 25/65 [10:09<17:13, 25.83s/it]2024-12-17 18:04:22,166 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 40%|█████████████████▏                         | 26/65 [10:33<16:19, 25.13s/it]2024-12-17 18:04:52,918 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 42%|█████████████████▊                         | 27/65 [11:04<16:59, 26.82s/it]2024-12-17 18:05:17,708 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 43%|██████████████████▌                        | 28/65 [11:29<16:09, 26.21s/it]2024-12-17 18:05:42,368 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 45%|███████████████████▏                       | 29/65 [11:53<15:26, 25.74s/it]2024-12-17 18:06:06,227 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 46%|███████████████████▊                       | 30/65 [12:17<14:41, 25.18s/it]2024-12-17 18:06:30,393 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 48%|████████████████████▌                      | 31/65 [12:41<14:05, 24.87s/it]2024-12-17 18:06:53,775 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 49%|█████████████████████▏                     | 32/65 [13:05<13:26, 24.43s/it]2024-12-17 18:07:20,882 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 51%|█████████████████████▊                     | 33/65 [13:32<13:27, 25.23s/it]2024-12-17 18:07:41,517 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 52%|██████████████████████▍                    | 34/65 [13:52<12:19, 23.85s/it]2024-12-17 18:08:03,325 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 54%|███████████████████████▏                   | 35/65 [14:14<11:37, 23.24s/it]2024-12-17 18:08:24,105 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 55%|███████████████████████▊                   | 36/65 [14:35<10:52, 22.50s/it]2024-12-17 18:08:48,012 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 57%|████████████████████████▍                  | 37/65 [14:59<10:41, 22.92s/it]2024-12-17 18:09:11,480 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 58%|█████████████████████████▏                 | 38/65 [15:22<10:23, 23.09s/it]2024-12-17 18:09:34,060 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 60%|█████████████████████████▊                 | 39/65 [15:45<09:56, 22.93s/it]2024-12-17 18:10:04,816 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 62%|██████████████████████████▍                | 40/65 [16:16<10:32, 25.28s/it]2024-12-17 18:10:31,989 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 63%|███████████████████████████                | 41/65 [16:43<10:20, 25.85s/it]2024-12-17 18:10:52,508 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 65%|███████████████████████████▊               | 42/65 [17:03<09:17, 24.25s/it]2024-12-17 18:11:15,556 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 66%|████████████████████████████▍              | 43/65 [17:26<08:45, 23.89s/it]2024-12-17 18:11:38,973 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 68%|█████████████████████████████              | 44/65 [17:50<08:18, 23.75s/it]2024-12-17 18:12:01,913 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 69%|█████████████████████████████▊             | 45/65 [18:13<07:50, 23.51s/it]2024-12-17 18:12:28,999 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 71%|██████████████████████████████▍            | 46/65 [18:40<07:47, 24.58s/it]2024-12-17 18:12:56,535 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 72%|███████████████████████████████            | 47/65 [19:07<07:38, 25.47s/it]2024-12-17 18:13:20,655 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 74%|███████████████████████████████▊           | 48/65 [19:31<07:06, 25.06s/it]2024-12-17 18:13:47,990 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 75%|████████████████████████████████▍          | 49/65 [19:59<06:51, 25.74s/it]2024-12-17 18:14:09,183 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 77%|█████████████████████████████████          | 50/65 [20:20<06:05, 24.38s/it]2024-12-17 18:14:35,920 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 78%|█████████████████████████████████▋         | 51/65 [20:47<05:51, 25.09s/it]2024-12-17 18:15:01,983 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 80%|██████████████████████████████████▍        | 52/65 [21:13<05:29, 25.38s/it]2024-12-17 18:15:25,234 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 82%|███████████████████████████████████        | 53/65 [21:36<04:56, 24.74s/it]2024-12-17 18:15:49,097 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 83%|███████████████████████████████████▋       | 54/65 [22:00<04:29, 24.48s/it]2024-12-17 18:16:10,362 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 85%|████████████████████████████████████▍      | 55/65 [22:21<03:55, 23.51s/it]2024-12-17 18:16:35,468 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 86%|█████████████████████████████████████      | 56/65 [22:46<03:35, 23.99s/it]2024-12-17 18:17:00,402 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 88%|█████████████████████████████████████▋     | 57/65 [23:11<03:14, 24.27s/it]2024-12-17 18:17:29,012 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 89%|██████████████████████████████████████▎    | 58/65 [23:40<02:59, 25.58s/it]2024-12-17 18:17:51,591 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 91%|███████████████████████████████████████    | 59/65 [24:02<02:28, 24.68s/it]2024-12-17 18:18:19,816 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 92%|███████████████████████████████████████▋   | 60/65 [24:31<02:08, 25.74s/it]2024-12-17 18:18:36,641 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 94%|████████████████████████████████████████▎  | 61/65 [24:47<01:32, 23.07s/it]2024-12-17 18:18:57,986 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 95%|█████████████████████████████████████████  | 62/65 [25:09<01:07, 22.55s/it]2024-12-17 18:19:20,691 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 97%|█████████████████████████████████████████▋ | 63/65 [25:31<00:45, 22.60s/it]2024-12-17 18:19:50,599 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 98%|██████████████████████████████████████████▎| 64/65 [26:01<00:24, 24.79s/it]2024-12-17 18:20:13,211 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "100%|███████████████████████████████████████████| 65/65 [26:24<00:00, 24.38s/it]\n"
     ]
    }
   ],
   "source": [
    "qa_dataset_base = generate_question_context_pairs(\n",
    "    nodes, num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "eb79ac7d-422f-43cb-b1a4-9f149baf1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset_base.queries = {key:value for key, value in qa_dataset_base.queries.items() if \"Here are\" not in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb2dee4d-f9e5-44da-95d5-31ece1c9c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset.queries = {key:value for key, value in qa_dataset.queries.items() if \"Here are\" not in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "330271f3-8526-4cde-90de-7e76c2503495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "metrics = [\"hit_rate\", \"mrr\"]\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cb79b25f-fafb-43cb-9b21-b1cf0d46f325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ea82b3b5-15c0-4d08-b9ae-06e6a23fabc2',\n",
       " 'What was one of the main challenges you faced when working with the IBM 1401 computer, and how did it affect your programming endeavors?')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(qa_dataset_base.queries.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1ffe0534-cbcd-48bd-aa77-901cedd5bd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b1e946c3-84f2-4133-aa23-ef693ba9ed31']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_base.relevant_docs['fbefe910-f921-4e08-a83b-c55de2197952']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "06669032-e375-4a32-b8eb-f4497d9159b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed.'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_base.corpus['b1e946c3-84f2-4133-aa23-ef693ba9ed31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b817a865-537b-464d-b6e0-247d7620b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What was one of the main challenges you faced when working with the IBM 1401 computer, and how did it affect your programming endeavors?\n",
      "Metrics: {'hit_rate': 1.0, 'mrr': 0.5}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset_base.queries.items())[0]\n",
    "sample_expected = qa_dataset_base.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1d08b792-f659-402a-8a77-95945b9f8159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.47it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.31it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 17.80it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 15.31it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.20it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.28it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 17.67it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.51it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.16it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.78it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.08it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.11it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.23it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.14it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.43it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.61it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.41it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.94it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.68it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.52it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.95it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.69it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.95it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.44it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.61it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.10it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 19.56it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.09it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.03it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.43it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.88it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.95it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.17it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.21it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.56it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.56it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.15it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.35it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.59it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.95it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 30.08it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.54it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 16.70it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  9.55it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 17.01it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 17.53it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 13.76it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.86it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.18it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.33it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.96it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.55it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.08it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.61it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 18.82it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.46it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 19.32it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.03it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.16it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.66it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.39it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 19.22it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.11it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# try it out on an entire dataset\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1b86dba6-599e-4663-b768-4b8baa8ade49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "24870f34-795a-4b44-b7a7-185f6e3d15da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base eval</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.470513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  retrievers  hit_rate       mrr\n",
       "0  base eval  0.769231  0.470513"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results(\"base eval\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc745eff-73c4-45fa-b53e-fb907d32c4db",
   "metadata": {},
   "source": [
    "## HybridRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb50e035-ad65-461e-8803-ccc5de49a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johny02/anaconda3/envs/local_rag_chat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-17 12:47:25,714 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2024-12-17 12:47:44,360 [INFO] sentence_transformers.SentenceTransformer: 2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "from local_rag_chat.core.loaders.simple_loader import SimpleLoader\n",
    "from local_rag_chat.core.embeddings.embedding_manager import EmbeddingManager\n",
    "\n",
    "embed_model = EmbeddingManager(model='BAAI/bge-small-en-v1.5').get_embedding()\n",
    "loader = SimpleLoader(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2cdd2-6d66-4673-9274-408d7ca43a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = loader.fit('./local_rag_chat/eval/data/paul_graham/paul_graham_essay.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0a44b77-876a-4097-83f4-edd85a810ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41df2e21-40f5-4fad-955d-e93aec7dfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.llm = llm.get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39f9a3b2-9144-45b5-93ba-026f60fd9562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:01<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from local_rag_chat.core.retrievers.hybrid_retriever import HybridRetriever\n",
    "hybrid_retriever = HybridRetriever(nodes=nodes,embed_model=embed_model,top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "994e1518-479f-4dc4-a0fd-bdae0b17d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.50it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = hybrid_retriever.retrieve(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838e840-5681-4b4c-a96b-4b094a283305",
   "metadata": {},
   "source": [
    "We'll try out retrieval over a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afdd568f-38ae-437f-a864-a994b076bdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 9ba7e089-d70a-4b53-a318-a5761ad84d5f<br>**Similarity:** 0.016666666666666666<br>**Text:** What I Worked On February 2021 Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 0431753d-36de-47fe-95be-50db5ddd3334<br>**Similarity:** 0.016666666666666666<br>**Text:** Now you not only had colleagues, but colleagues who understood the problems you were facing and could tell you how they were solving them.\n",
       "As YC grew, we started to notice other advantages of scale. The alumni became a tight community, dedicated to helping one another, and especially the current  batch, whose shoes they remembered being in. We also noticed that the startups were becoming one another's customers. We used to refer jokingly to the \"YC GDP,\" but as YC grows this becomes less and less of a joke.\n",
       "Now lots of startups get their initial set of customers almost entirely from among their batchmates.\n",
       "I had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited about it, it started to take up a lot more than a third of my attention. But for the first few years I was still able to work on other things.\n",
       "In the summer of 2006, Robert and I started working on a new version of Arc. Thi...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 7e6e3828-6619-45e4-88a9-9f9a00c4706e<br>**Similarity:** 0.01639344262295082<br>**Text:** It's called Yorkville, and that was my new home. Now I was a New York artist — in the strictly technical sense of making paintings and living in New York.\n",
       "I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a54af-7f11-4cd2-bc1e-9609e3fd6e5b",
   "metadata": {},
   "source": [
    "### Build an Evaluation dataset of (query, context) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4000512-19e6-4173-99b8-45e5d8dc3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd471415-ed5b-4665-ba0e-c5d4df1159f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/39 [00:00<?, ?it/s]2024-12-17 13:36:53,361 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  3%|█▏                                          | 1/39 [00:22<14:16, 22.54s/it]2024-12-17 13:37:13,052 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  5%|██▎                                         | 2/39 [00:42<12:51, 20.86s/it]2024-12-17 13:37:35,953 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  8%|███▍                                        | 3/39 [01:05<13:04, 21.79s/it]2024-12-17 13:38:06,452 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 10%|████▌                                       | 4/39 [01:35<14:43, 25.23s/it]2024-12-17 13:38:21,616 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 13%|█████▋                                      | 5/39 [01:50<12:14, 21.60s/it]2024-12-17 13:38:30,752 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 15%|██████▊                                     | 6/39 [01:59<09:32, 17.36s/it]2024-12-17 13:39:03,687 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 18%|███████▉                                    | 7/39 [02:32<11:58, 22.45s/it]2024-12-17 13:39:10,874 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 21%|█████████                                   | 8/39 [02:40<09:05, 17.59s/it]2024-12-17 13:39:48,342 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 23%|██████████▏                                 | 9/39 [03:17<11:54, 23.81s/it]2024-12-17 13:40:31,653 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 26%|███████████                                | 10/39 [04:00<14:24, 29.83s/it]2024-12-17 13:40:40,881 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 28%|████████████▏                              | 11/39 [04:10<10:58, 23.52s/it]2024-12-17 13:40:52,338 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 31%|█████████████▏                             | 12/39 [04:21<08:55, 19.85s/it]2024-12-17 13:41:06,257 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 33%|██████████████▎                            | 13/39 [04:35<07:49, 18.06s/it]2024-12-17 13:41:15,447 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 36%|███████████████▍                           | 14/39 [04:44<06:24, 15.38s/it]2024-12-17 13:41:27,683 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 38%|████████████████▌                          | 15/39 [04:56<05:46, 14.43s/it]2024-12-17 13:41:38,653 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 41%|█████████████████▋                         | 16/39 [05:07<05:07, 13.39s/it]2024-12-17 13:42:03,107 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 44%|██████████████████▋                        | 17/39 [05:32<06:07, 16.72s/it]2024-12-17 13:42:12,836 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 46%|███████████████████▊                       | 18/39 [05:42<05:06, 14.62s/it]2024-12-17 13:42:42,016 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 49%|████████████████████▉                      | 19/39 [06:11<06:19, 18.99s/it]2024-12-17 13:42:58,890 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 51%|██████████████████████                     | 20/39 [06:28<05:48, 18.35s/it]2024-12-17 13:43:59,471 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 54%|███████████████████████▏                   | 21/39 [07:28<09:18, 31.03s/it]2024-12-17 13:44:33,988 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 56%|████████████████████████▎                  | 22/39 [08:03<09:05, 32.08s/it]2024-12-17 13:44:46,006 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 59%|█████████████████████████▎                 | 23/39 [08:15<06:56, 26.06s/it]2024-12-17 13:44:56,298 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 62%|██████████████████████████▍                | 24/39 [08:25<05:19, 21.33s/it]2024-12-17 13:45:04,240 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 64%|███████████████████████████▌               | 25/39 [08:33<04:02, 17.31s/it]2024-12-17 13:45:17,496 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 67%|████████████████████████████▋              | 26/39 [08:46<03:29, 16.09s/it]2024-12-17 13:45:30,773 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 69%|█████████████████████████████▊             | 27/39 [08:59<03:03, 15.25s/it]2024-12-17 13:45:39,859 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 72%|██████████████████████████████▊            | 28/39 [09:09<02:27, 13.40s/it]2024-12-17 13:46:38,208 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 74%|███████████████████████████████▉           | 29/39 [10:07<04:28, 26.89s/it]2024-12-17 13:47:23,569 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 77%|█████████████████████████████████          | 30/39 [10:52<04:51, 32.43s/it]2024-12-17 13:47:41,228 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 79%|██████████████████████████████████▏        | 31/39 [11:10<03:43, 28.00s/it]2024-12-17 13:48:18,698 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 82%|███████████████████████████████████▎       | 32/39 [11:47<03:35, 30.84s/it]2024-12-17 13:48:32,017 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 85%|████████████████████████████████████▍      | 33/39 [12:01<02:33, 25.58s/it]2024-12-17 13:48:53,788 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 87%|█████████████████████████████████████▍     | 34/39 [12:22<02:02, 24.44s/it]2024-12-17 13:49:07,899 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 90%|██████████████████████████████████████▌    | 35/39 [12:37<01:25, 21.34s/it]2024-12-17 13:49:16,187 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 92%|███████████████████████████████████████▋   | 36/39 [12:45<00:52, 17.42s/it]2024-12-17 13:49:40,743 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 95%|████████████████████████████████████████▊  | 37/39 [13:09<00:39, 19.56s/it]2024-12-17 13:50:15,145 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 97%|█████████████████████████████████████████▉ | 38/39 [13:44<00:24, 24.02s/it]2024-12-17 13:51:12,565 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "100%|███████████████████████████████████████████| 39/39 [14:41<00:00, 22.61s/it]\n"
     ]
    }
   ],
   "source": [
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50e4888d-79ca-4cd6-9400-d969fff64a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are two potential questions based on the context information:'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(qa_dataset.queries.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d156cb2-3622-4dbf-a46c-39147e9f0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset.save_json(\"pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7c809-ebe8-46ee-b5a0-689eab58c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] load\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a0b3e59-9d7d-490d-8e74-a29286048951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "metrics = [\"hit_rate\", \"mrr\"]\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=hybrid_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca222c29-2882-4d23-b609-cfc8061dac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bb32d546-4d02-4b1c-b890-4476d8930efe',\n",
       " \"What type of computer was used by the narrator's school district, and where was it located?\")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(qa_dataset.queries.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "00e4f454-4d53-4fe3-b32b-95c93f7178fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 33.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What type of computer was used by the narrator's school district, and where was it located?\n",
      "Metrics: {'hit_rate': 1.0, 'mrr': 1.0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13995905-fb12-41ec-a48a-516266696ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 31.72it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 18.64it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.36it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.66it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.58it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.35it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.07it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.63it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.28it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.55it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.35it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.34it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.96it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.37it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.96it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.67it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.93it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.88it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.83it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.27it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 21.12it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.75it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.02it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.14it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.41it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.86it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.56it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.16it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.62it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.23it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.20it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.52it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.36it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.16it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.45it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.95it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.89it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 20.87it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# try it out on an entire dataset\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "09fccaf8-c33f-4ebb-bf39-13ebf89de1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "176cd185-e66b-4ce3-870e-8ce9aac44fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hybrid eval</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.735043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    retrievers  hit_rate       mrr\n",
       "0  hybrid eval  0.794872  0.735043"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results(\"hybrid eval\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0801ee-a850-42c8-96a0-5a2b1374b873",
   "metadata": {},
   "source": [
    "## HybridRetriever without SemanticSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28cf93f-a521-41d1-84d2-facc29a24806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_rag_chat.core.loaders.simple_loader import SimpleLoader\n",
    "loader = SimpleLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "359ccd16-33a0-49d3-a059-c1646b35eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johny02/anaconda3/envs/local_rag_chat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-18 15:22:52,866 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2024-12-18 15:23:04,465 [INFO] sentence_transformers.SentenceTransformer: 2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "from local_rag_chat.core.embeddings.embedding_manager import EmbeddingManager\n",
    "\n",
    "embed_model = EmbeddingManager(model='BAAI/bge-small-en-v1.5').get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d30ef0e-e89b-4790-a264-1c125bca7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = loader.fit('./local_rag_chat/eval/data/paul_graham/paul_graham_essay.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb8dc36-231e-4fc4-b648-7a3d989b9cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from local_rag_chat.core.retrievers.hybrid_retriever import HybridRetriever\n",
    "hybrid_retriever = HybridRetriever(nodes=nodes,embed_model=embed_model,top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58835ac-bd52-4a0b-bcf7-82896b2619c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.15it/s]\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = hybrid_retriever.retrieve(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e69833-2c4e-4c04-879b-b40d401c4628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 17837ef9-6a60-4c57-9e17-8e492036675f<br>**Similarity:** 0.016666666666666666<br>**Text:** What I Worked On February 2021 Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "The language we used was an early version of Fortran.\n",
       "You had to type programs on punch cards, then stack them in the ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 61d1f2fb-1a23-4230-a79a-14aa64d27763<br>**Similarity:** 0.016666666666666666<br>**Text:** If we'd been a startup I was advising at Y Combinator, I would have said: Stop being so stressed out, because you're doing fine. You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.\n",
       "Alas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish. So we didn't reach breakeven until about when Yahoo bought us in the summer of 1998.\n",
       "Which in turn meant we were at the mercy of investors for the entire life of the company. And since both we and our investors were noobs at startups, the result was a mess even by startup standards.\n",
       "It was a huge relief when Yahoo bought us. In  principle our Viaweb stock was valuable. It was a share in a business that was profitable and growing rapidly. But it didn't feel very valuable to me; I had no idea how to value a bu...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** af908143-471a-45d9-bf85-9286b97e874f<br>**Similarity:** 0.01639344262295082<br>**Text:** Afterward I put a postscript file of this talk online, on paulgraham.com, which I'd created years before using Viaweb but had never used for anything. In one day it got 30,000 page views. What on earth had happened?\n",
       "The referring urls showed that someone had posted it on Slashdot. [10] Wow, I thought, there's an audience. If I write something and put it on the web, anyone can read it.\n",
       "That may seem obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce monsters known as editors.\n",
       "The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now anyone could publish anything.\n",
       "This had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to underst...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149a71c2-fc42-4ecd-9cfd-9d8e0623cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87caf642-e136-4a83-b19e-8b4dfcff382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]2024-12-18 15:25:26,664 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  2%|▏         | 1/42 [00:29<19:52, 29.08s/it]2024-12-18 15:25:50,636 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  5%|▍         | 2/42 [00:52<17:20, 26.02s/it]2024-12-18 15:26:20,563 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "  7%|▋         | 3/42 [01:22<18:04, 27.81s/it]2024-12-18 15:26:43,509 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 10%|▉         | 4/42 [01:45<16:23, 25.89s/it]2024-12-18 15:27:06,425 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 12%|█▏        | 5/42 [02:08<15:18, 24.82s/it]2024-12-18 15:27:32,341 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 14%|█▍        | 6/42 [02:34<15:06, 25.19s/it]2024-12-18 15:27:54,859 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 7/42 [02:57<14:11, 24.32s/it]2024-12-18 15:28:21,611 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 19%|█▉        | 8/42 [03:23<14:13, 25.09s/it]2024-12-18 15:28:42,076 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 21%|██▏       | 9/42 [03:44<13:00, 23.65s/it]2024-12-18 15:29:04,021 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 24%|██▍       | 10/42 [04:06<12:19, 23.12s/it]2024-12-18 15:29:28,605 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 26%|██▌       | 11/42 [04:30<12:10, 23.57s/it]2024-12-18 15:29:53,076 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 29%|██▊       | 12/42 [04:55<11:55, 23.84s/it]2024-12-18 15:30:21,557 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 31%|███       | 13/42 [05:23<12:12, 25.25s/it]2024-12-18 15:30:43,685 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 14/42 [05:46<11:20, 24.31s/it]2024-12-18 15:31:07,130 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 36%|███▌      | 15/42 [06:09<10:49, 24.05s/it]2024-12-18 15:31:28,327 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 38%|███▊      | 16/42 [06:30<10:02, 23.19s/it]2024-12-18 15:31:52,457 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 17/42 [06:54<09:46, 23.47s/it]2024-12-18 15:32:15,683 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 43%|████▎     | 18/42 [07:18<09:21, 23.40s/it]2024-12-18 15:32:38,389 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 45%|████▌     | 19/42 [07:40<08:53, 23.19s/it]2024-12-18 15:33:07,498 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 48%|████▊     | 20/42 [08:09<09:09, 24.97s/it]2024-12-18 15:33:38,210 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 21/42 [08:40<09:20, 26.69s/it]2024-12-18 15:34:04,396 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 52%|█████▏    | 22/42 [09:06<08:50, 26.54s/it]2024-12-18 15:34:28,747 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 55%|█████▍    | 23/42 [09:31<08:11, 25.88s/it]2024-12-18 15:34:57,843 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 57%|█████▋    | 24/42 [10:00<08:03, 26.85s/it]2024-12-18 15:35:24,714 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 60%|█████▉    | 25/42 [10:27<07:36, 26.86s/it]2024-12-18 15:35:51,387 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 62%|██████▏   | 26/42 [10:53<07:08, 26.80s/it]2024-12-18 15:36:17,194 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 64%|██████▍   | 27/42 [11:19<06:37, 26.50s/it]2024-12-18 15:36:44,270 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 28/42 [11:46<06:13, 26.68s/it]2024-12-18 15:37:06,813 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 69%|██████▉   | 29/42 [12:09<05:30, 25.43s/it]2024-12-18 15:37:29,838 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 71%|███████▏  | 30/42 [12:32<04:56, 24.71s/it]2024-12-18 15:37:56,079 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 74%|███████▍  | 31/42 [12:58<04:36, 25.17s/it]2024-12-18 15:38:20,139 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 76%|███████▌  | 32/42 [13:22<04:08, 24.84s/it]2024-12-18 15:38:47,320 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 79%|███████▊  | 33/42 [13:49<03:49, 25.54s/it]2024-12-18 15:39:10,686 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 81%|████████  | 34/42 [14:13<03:19, 24.89s/it]2024-12-18 15:39:33,983 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 83%|████████▎ | 35/42 [14:36<02:50, 24.41s/it]2024-12-18 15:40:02,415 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 86%|████████▌ | 36/42 [15:04<02:33, 25.62s/it]2024-12-18 15:40:32,971 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 88%|████████▊ | 37/42 [15:35<02:15, 27.10s/it]2024-12-18 15:40:56,076 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 90%|█████████ | 38/42 [15:58<01:43, 25.90s/it]2024-12-18 15:41:21,377 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 93%|█████████▎| 39/42 [16:23<01:17, 25.72s/it]2024-12-18 15:41:43,760 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 95%|█████████▌| 40/42 [16:46<00:49, 24.72s/it]2024-12-18 15:42:06,032 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      " 98%|█████████▊| 41/42 [17:08<00:23, 23.98s/it]2024-12-18 15:42:24,025 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 42/42 [17:26<00:00, 24.91s/it]\n"
     ]
    }
   ],
   "source": [
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dce5e04-810c-4946-b2b0-6d4d5a610a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset.queries = {key:value for key, value in qa_dataset.queries.items() if \"Here are\" not in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "161f0e21-4b51-4387-a194-78f106495661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "metrics = [\"hit_rate\", \"mrr\"]\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=hybrid_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b7b3a28-7355-4b5c-9420-18da08cefe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What was the primary input method used to program the IBM 1401, and what were some of its limitations?\n",
      "Metrics: {'hit_rate': 1.0, 'mrr': 1.0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61228d46-2bec-4a9b-bec0-03d0defe7bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# try it out on an entire dataset\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03860c2e-50c3-4b00-9dc5-3bdba575c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "105228b8-a2fd-487a-97d8-a4c732a86a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hybrid without semantic split</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.869048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      retrievers  hit_rate       mrr\n",
       "0  hybrid without semantic split  0.952381  0.869048"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results(\"hybrid without semantic split\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09669c92-9ee7-406b-a7e7-5f3ce8488a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag Chatbot",
   "language": "python",
   "name": "local_rag_chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
