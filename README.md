# RAG chat with PDF
This project implements a Retrieval-Augmented Generation (RAG) chatbot capable of interacting with PDF documents. It leverages **LlamaIndex** for document indexing and retrieval and **Ollama** for enhanced conversational AI capabilities.

![img.png](img/img.png)
---
### Table of Contents
1. [Features](#features-)
2. [Setup](#setup-)
    - [Build from Source](#build-from-source-)
    - [Deploy with Docker](#deploy-with-docker-)

## Features üöÄ

| ü§ñ Model Support                  | Implemented | Description                                             |
| --------------------------------- | ---------- | ------------------------------------------------------- |
| Ollama (e.g. Llama3)              | ‚úÖ         | Local Embedding and Generation Models powered by Ollama |
| OpenAI (e.g. GPT4)                | ‚úÖ         | Embedding and Generation Models by OpenAI               |

| ü§ñ Embedding Support | Implemented | Description                              |
| -------------------- | ---------- | ---------------------------------------- |
| Ollama               | ‚úÖ         | Local Embedding Models powered by Ollama |
| OpenAI               |           | Embedding Models by OpenAI               |

| üìÅ Data Support    | Implemented | Description                                   |
|--------------------|------------|-----------------------------------------------|
| PDF Ingestion      | ‚úÖ          | Import PDF                                    |
| CSV/XLSX Ingestion | planned ‚è±Ô∏è | Import Table Data into Verba                  |
| .DOCX              | planned ‚è±Ô∏è | Import .docx files                            |
| Multi-Modal        | planned ‚è±Ô∏è | Import and Transcribe Audio through AssemblyAI |

| ‚ú® RAG Features        | Implemented   | Description                                                                 |
|-----------------------|---------------|-----------------------------------------------------------------------------|
| Hybrid Search         | ‚úÖ             | Semantic Search combined with Keyword Search                                |
| Router                | ‚úÖ             | Router Retriever base on your query (summary and specific contexts)         |
| Query Transformations | planned ‚è±Ô∏è     | Enhance retrieval by refining queries for improved relevance and accuracy.  |
| Filtering             | ‚úÖ             | Apply Filters (e.g. documents, document types etc.) before performing RAG   |
| Reranking             | ‚úÖ             | Rerank results based on context for improved results                        |
| RAG Evaluation        | ‚úÖ             | Interface for Evaluating RAG pipelines                                      |
| Agentic RAG           | out of scope ‚ùå | Agentic RAG pipelines                                                       |
| Graph RAG             | out of scope ‚ùå | Graph-based RAG pipelines                                                   |

| üó°Ô∏è Chunking Techniques | Implemented | Description                                     |
| ---------------------- | ----------- |-------------------------------------------------|
| Sentence               | ‚úÖ          | Chunk by Sentence                               |
| Semantic               | ‚úÖ          | Chunk and group by semantic sentence similarity |

## Setup üõ†Ô∏è
### Build from Source üèóÔ∏è
#### 1. Clone the repository
```
git clone https://github.com/johnPa02/local-rag-chat.git
cd local-rag-chat
```
#### 2. Install the dependencies
```
pip install poetry
poetry install
```
#### 3. Create a `.env` file in the root directory and add the following environment variables
```
cp .env.example .env
```
#### 4. Ollama
    - This project supports Ollama models. Download and Install Ollama on your device (https://ollama.com/download). Make sure to install your preferred LLM using `ollama run <model>`.
    
    Tested with `llama3`, `llama3:70b` and `mistral`. The bigger models generally perform better, but need more computational power.
    
    > Make sure Ollama Server runs in the background and that you don't ingest documents with different ollama models since their vector dimension can vary that will lead to errors
    
    You can verify that by running the following command
    
    ```
    ollama run llama3
    ```
#### 5. Run the application
- Before running the application, change the `OLLAMA_BASE_URL` in the configs.py file to `http://ollama_server:11434`
  ```
  python app.py
  ``` 
### Deploy with Docker üê≥
Run the following commands to deploy the application using Docker
```
docker-compose up --build
```